{"cells":[{"cell_type":"markdown","source":["**<span style=\"color:red\">Warning</span>**: Do \"File -> Save a copy in Drive\" before you start modifying the notebook, otherwise your modifications will not be saved."],"metadata":{"id":"YPB2EK9FEiMp"},"id":"YPB2EK9FEiMp"},{"cell_type":"markdown","source":["# BERT for Sentiment Analysis "],"metadata":{"id":"_UFwj3KNufA4"},"id":"_UFwj3KNufA4"},{"cell_type":"code","source":["!pip install transformers\n","!pip install jupyter_black"],"metadata":{"id":"IlFyycx9qLTP"},"id":"IlFyycx9qLTP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import jupyter_black\n","\n","jupyter_black.load(lab=False, line_length=100)"],"metadata":{"id":"hY-Kd7VHoKAl","executionInfo":{"status":"ok","timestamp":1676641756245,"user_tz":-60,"elapsed":678,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}}},"id":"hY-Kd7VHoKAl","execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"6e14ce48","metadata":{"id":"6e14ce48"},"outputs":[],"source":["import transformers\n","import tensorflow as tf"]},{"cell_type":"markdown","source":["## Downloading large review movie dataset (50000 reviews in train, 50000 reviews in test)"],"metadata":{"id":"BtKn-7auvbsh"},"id":"BtKn-7auvbsh"},{"cell_type":"code","source":["!wget https://thome.isir.upmc.fr/classes/RITAL/json_pol"],"metadata":{"id":"a5i1H9--qZsC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676640876346,"user_tz":-60,"elapsed":6790,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"db6751a7-fdc0-45f9-b4cf-e02709f30773"},"id":"a5i1H9--qZsC","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-02-17 13:34:28--  https://thome.isir.upmc.fr/classes/RITAL/json_pol\n","Resolving thome.isir.upmc.fr (thome.isir.upmc.fr)... 134.157.18.247\n","Connecting to thome.isir.upmc.fr (thome.isir.upmc.fr)|134.157.18.247|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 66110051 (63M) [application/octet-stream]\n","Saving to: ‘json_pol.1’\n","\n","json_pol.1          100%[===================>]  63.05M  14.8MB/s    in 5.4s    \n","\n","2023-02-17 13:34:35 (11.8 MB/s) - ‘json_pol.1’ saved [66110051/66110051]\n","\n"]}]},{"cell_type":"code","execution_count":18,"id":"6c40511e","metadata":{"id":"6c40511e","colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"status":"ok","timestamp":1676641759198,"user_tz":-60,"elapsed":1226,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"1d37ea8b-e61f-4662-d902-284f00eb8a42"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","            (function() {\n","                jb_set_cell(\"import json\\nfrom collections import Counter\\n\\n# Loading json\\nwith open(\\\"./json_pol\\\", encoding=\\\"utf-8\\\") as f:\\n    data = f.readlines()\\n    json_data = json.loads(data[0])\\n    train = json_data[\\\"train\\\"]\\n    test = json_data[\\\"test\\\"]\\n\\n\\n# Quick Check\\ncounter_train = Counter((x[1] for x in train))\\ncounter_test = Counter((x[1] for x in test))\\nprint(\\\"Number of train reviews : \\\", len(train))\\nprint(\\\"----> # of positive : \\\", counter_train[1])\\nprint(\\\"----> # of negative : \\\", counter_train[0])\\nprint(\\\"\\\")\\nprint(train[0])\\nprint(\\\"\\\")\\nprint(\\\"Number of test reviews : \\\", len(test))\\nprint(\\\"----> # of positive : \\\", counter_test[1])\\nprint(\\\"----> # of negative : \\\", counter_test[0])\\n\\nprint(\\\"\\\")\\nprint(test[0])\\nprint(\\\"\\\")\")\n","            })();\n","            "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Number of train reviews :  25000\n","----> # of positive :  12500\n","----> # of negative :  12500\n","\n","[\"The undoubted highlight of this movie is Peter O'Toole's performance. In turn wildly comical and terribly terribly tragic. Does anybody do it better than O'Toole? I don't think so. What a great face that man has!<br /><br />The story is an odd one and quite disturbing and emotionally intense in parts (especially toward the end) but it is also oddly touching and does succeed on many levels. However, I felt the film basically revolved around Peter O'Toole's luminous performance and I'm sure I wouldn't have enjoyed it even half as much if he hadn't been in it.\", 1]\n","\n","Number of test reviews :  25000\n","----> # of positive :  12500\n","----> # of negative :  12500\n","\n","['Although credit should have been given to Dr. Seuess for stealing the story-line of \"Horton Hatches The Egg\", this was a fine film. It touched both the emotions and the intellect. Due especially to the incredible performance of seven year old Justin Henry and a script that was sympathetic to each character (and each one\\'s predicament), the thought provoking elements linger long after the tear jerking ones are over. Overall, superior acting from a solid cast, excellent directing, and a very powerful script. The right touches of humor throughout help keep a \"heavy\" subject from becoming tedious or difficult to sit through. Lastly, this film stands the test of time and seems in no way dated, decades after it was released.', 1]\n","\n"]}],"source":["import json\n","from collections import Counter\n","\n","# Loading json\n","with open(\"./json_pol\",encoding=\"utf-8\") as f:\n","    data = f.readlines()\n","    json_data = json.loads(data[0])\n","    train = json_data[\"train\"]\n","    test = json_data[\"test\"]\n","    \n","\n","# Quick Check\n","counter_train = Counter((x[1] for x in train))\n","counter_test = Counter((x[1] for x in test))\n","print(\"Number of train reviews : \", len(train))\n","print(\"----> # of positive : \", counter_train[1])\n","print(\"----> # of negative : \", counter_train[0])\n","print(\"\")\n","print(train[0])\n","print(\"\")\n","print(\"Number of test reviews : \",len(test))\n","print(\"----> # of positive : \", counter_test[1])\n","print(\"----> # of negative : \", counter_test[0])\n","\n","print(\"\")\n","print(test[0])\n","print(\"\")\n"]},{"cell_type":"markdown","source":["## Getting the Tokenizer"],"metadata":{"id":"0dsRcmntwfOH"},"id":"0dsRcmntwfOH"},{"cell_type":"code","execution_count":19,"id":"4381e234","metadata":{"id":"4381e234","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1676641764824,"user_tz":-60,"elapsed":1215,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"bd44f67a-ba31-42fd-f7c7-9f2ff352263c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","            (function() {\n","                jb_set_cell(\"from transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\\"textattack/bert-base-uncased-yelp-polarity\\\")\")\n","            })();\n","            "]},"metadata":{}}],"source":["from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n"]},{"cell_type":"markdown","source":["### Experiment the Tokenizer on the first train review"],"metadata":{"id":"GPTSFflDwkoh"},"id":"GPTSFflDwkoh"},{"cell_type":"code","execution_count":20,"id":"ddc98b0c","metadata":{"id":"ddc98b0c","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1676641768337,"user_tz":-60,"elapsed":739,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"909baf14-649d-491d-dbf7-7c9c5f069cc7"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","            (function() {\n","                jb_set_cell(\"maxL = 512  # Max length of the sequence\\n\\nstring_tokenized = tokenizer.encode_plus(\\n    train[0][0],\\n    return_tensors=\\\"pt\\\",\\n    add_special_tokens=True,  # add '[CLS]' and '[SEP]'\\n    max_length=maxL,  # set max length\\n    truncation=True,  # truncate longer messages\\n    # pad_to_max_length=True\\n    padding=\\\"max_length\\\",  # add padding\\n    return_attention_mask=True,\\n)\")\n","            })();\n","            "]},"metadata":{}}],"source":["maxL = 512 # Max length of the sequence\n","\n","string_tokenized = tokenizer.encode_plus(train[0][0], return_tensors=\"pt\", \n","                                        add_special_tokens=True,  # add '[CLS]' and '[SEP]'\n","                            max_length=maxL,  # set max length\n","                            truncation=True,  # truncate longer messages\n","                            #pad_to_max_length=True\n","                            padding='max_length',  # add padding\n","                            return_attention_mask=True)"]},{"cell_type":"markdown","source":["The output of the tokenizer string_tokenized (class BatchEncoding) returns two elements:\n","\n","\n","*   string_tokenized['input_ids']: the index of each token in the dictionary\n","*   string_tokenized['attention_mask']: a binary mask (0 to ignore the token, 1 to consider it). This is because we need tensor a fixed length and we have reviews with a variable number of words\n","\n"],"metadata":{"id":"v910nrNVx33z"},"id":"v910nrNVx33z"},{"cell_type":"code","source":["print(\"Index:\\n\", string_tokenized['input_ids'])\n","print(\"Mask:\\n\", string_tokenized['attention_mask'])"],"metadata":{"id":"sagULO5nx-3H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676639698622,"user_tz":-60,"elapsed":623,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"36c3db25-ffe7-4a59-fed5-42f5ab14653a"},"id":"sagULO5nx-3H","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index:\n"," tensor([[  101,  1996, 25672, 12083,  3064, 12944,  1997,  2023,  3185,  2003,\n","          2848,  1051,  1005,  6994,  2063,  1005,  1055,  2836,  1012,  1999,\n","          2735, 13544, 29257,  1998, 16668, 16668, 13800,  1012,  2515, 10334,\n","          2079,  2009,  2488,  2084,  1051,  1005,  6994,  2063,  1029,  1045,\n","          2123,  1005,  1056,  2228,  2061,  1012,  2054,  1037,  2307,  2227,\n","          2008,  2158,  2038,   999,  1026,  7987,  1013,  1028,  1026,  7987,\n","          1013,  1028,  1996,  2466,  2003,  2019,  5976,  2028,  1998,  3243,\n","         14888,  1998, 14868,  6387,  1999,  3033,  1006,  2926,  2646,  1996,\n","          2203,  1007,  2021,  2009,  2003,  2036, 15056,  7244,  1998,  2515,\n","          9510,  2006,  2116,  3798,  1012,  2174,  1010,  1045,  2371,  1996,\n","          2143, 10468,  7065, 16116,  2105,  2848,  1051,  1005,  6994,  2063,\n","          1005,  1055, 25567,  2836,  1998,  1045,  1005,  1049,  2469,  1045,\n","          2876,  1005,  1056,  2031,  5632,  2009,  2130,  2431,  2004,  2172,\n","          2065,  2002,  2910,  1005,  1056,  2042,  1999,  2009,  1012,   102,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0]])\n","Mask:\n"," tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]])\n"]}]},{"cell_type":"markdown","source":["# Lets download a BERT model for word embedding"],"metadata":{"id":"TunnUo2p0FT9"},"id":"TunnUo2p0FT9"},{"cell_type":"code","execution_count":31,"id":"9fc3269c","metadata":{"id":"9fc3269c","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1676642445813,"user_tz":-60,"elapsed":2865,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"7cf7d60e-9bfd-4b7a-aeea-3972668fff80"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","            (function() {\n","                jb_set_cell(\"from transformers import BertForSequenceClassification\\n\\nmodel = BertForSequenceClassification.from_pretrained(\\\"textattack/bert-base-uncased-yelp-polarity\\\")\")\n","            })();\n","            "]},"metadata":{}}],"source":["from transformers import BertForSequenceClassification\n","model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")"]},{"cell_type":"code","execution_count":32,"id":"1155599b","metadata":{"id":"1155599b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676642445814,"user_tz":-60,"elapsed":15,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"f4f9e417-90cb-4f63-c7a9-6cd4422d793b"},"outputs":[{"output_type":"stream","name":"stdout","text":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")\n"]}],"source":["print(model)"]},{"cell_type":"markdown","source":["**You can use the BERT model for directly predicting polarity.** Let us apply that on the first review which has been tokenized with string_tokenized."],"metadata":{"id":"23VAnnrU0QO5"},"id":"23VAnnrU0QO5"},{"cell_type":"code","execution_count":33,"id":"a15ab0db","metadata":{"id":"a15ab0db","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1676642447668,"user_tz":-60,"elapsed":1146,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"e73a8b12-a229-4a77-8079-e80ea674c51b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","            (function() {\n","                jb_set_cell(\"# Some preliminary test\\nimport torch\\nimport numpy as np\\n\\nb_input_ids = string_tokenized[\\\"input_ids\\\"]\\nb_input_mask = string_tokenized[\\\"attention_mask\\\"]\\n\\nmodel.eval()\\n\\noutput = model(input_ids=b_input_ids, attention_mask=b_input_mask, output_hidden_states=True)\\nprint(output.logits)  # The output of the logit of the two classes (polarity pos/neg)\\nlast_hidden_states = output.hidden_states[\\n    -1\\n]  # The last layer before the class prediction: tensor of size nBatch (1 here) x MaxL (512) x temb (768)\\nprint(last_hidden_states.shape)  # length of sequence (512) * length of embedding (768)\\nprint(\\n    last_hidden_states[0, 0, 1:10]\\n)  # The first 10 values (out of 768) of the first elements (=[CLS] TOKEN)\\nprint(f\\\" norm cls token = {np.linalg.norm(last_hidden_states.detach().numpy()[0,0,:])}\\\")\")\n","            })();\n","            "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["tensor([[ 4.4636, -4.0851]], grad_fn=<AddmmBackward0>)\n","torch.Size([1, 512, 768])\n","tensor([ 0.8941, -0.4308,  0.6871, -0.2124,  0.0930,  1.1323, -0.7455, -0.1118,\n","        -0.4200], grad_fn=<SliceBackward0>)\n"," norm cls token = 17.2490291595459\n"]}],"source":["# Some preliminary test\n","import torch\n","import numpy as np\n","b_input_ids = string_tokenized['input_ids']\n","b_input_mask = string_tokenized['attention_mask']\n","\n","model.eval()\n","\n","output = model(input_ids=b_input_ids,attention_mask=b_input_mask, output_hidden_states=True)\n","print(output.logits) # The output of the logit of the two classes (polarity pos/neg)  \n","last_hidden_states = output.hidden_states[-1] # The last layer before the class prediction: tensor of size nBatch (1 here) x MaxL (512) x temb (768)\n","print(last_hidden_states.shape) # length of sequence (512) * length of embedding (768)\n","print(last_hidden_states[0,0,1:10]) # The first 10 values (out of 768) of the first elements (=[CLS] TOKEN)\n","print(f\" norm cls token = {np.linalg.norm(last_hidden_states.detach().numpy()[0,0,:])}\") "]},{"cell_type":"markdown","source":["Logit = valeur d'un score avant qu'il soit probabilisé."],"metadata":{"id":"XppjBqXzl4q8"},"id":"XppjBqXzl4q8"},{"cell_type":"markdown","source":["# Let's tokenize the whole dataset "],"metadata":{"id":"hdBab99u4HNm"},"id":"hdBab99u4HNm"},{"cell_type":"markdown","source":["Maintenant, on refait pareil, mais pour l'ensemble du jeu de données."],"metadata":{"id":"JoiU1mEuqdIE"},"id":"JoiU1mEuqdIE"},{"cell_type":"code","execution_count":45,"id":"d40ca05a","metadata":{"id":"d40ca05a","colab":{"base_uri":"https://localhost:8080/","height":100},"executionInfo":{"status":"ok","timestamp":1676645446089,"user_tz":-60,"elapsed":70203,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"1842a898-bb42-47c0-a157-3035f96df661"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","            (function() {\n","                jb_set_cell(\"import numpy as np\\n\\nmaxL = 512\\ntemb = 768\\n\\ninputs_tokens_train = []\\nattention_masks_train = []\\n\\nprint(\\\"tokenizing train dataset...\\\")\\nfor i in tqdm(range(len(train))):\\n    # if(i%2500==0):\\n    #     print(i)\\n    string_tokenized = tokenizer.encode_plus(\\n        train[i][0],\\n        return_tensors=\\\"pt\\\",\\n        add_special_tokens=True,  # add '[CLS]' and '[SEP]'\\n        max_length=maxL,  # set max length\\n        truncation=True,  # truncate longer messages\\n        # pad_to_max_length=True\\n        padding=\\\"max_length\\\",  # add padding\\n        return_attention_mask=True,\\n    )\\n\\n    # APPEND inputs token and input masks. YOUR CODE HERE\\n    inputs_tokens_train.append(string_tokenized[\\\"input_ids\\\"])\\n    attention_masks_train.append(string_tokenized[\\\"attention_mask\\\"])\\n\\ninputs_tokens_test = []\\nattention_masks_test = []\\n\\nprint(\\\"tokenizing test dataset...\\\")\\nfor i in tqdm(range(len(test))):\\n    # if(i%2500==0):\\n    #     print(i)\\n    string_tokenized = tokenizer.encode_plus(\\n        test[i][0],\\n        return_tensors=\\\"pt\\\",\\n        add_special_tokens=True,  # add '[CLS]' and '[SEP]'\\n        max_length=maxL,  # set max length\\n        truncation=True,  # truncate longer messages\\n        # pad_to_max_length=True\\n        padding=\\\"max_length\\\",  # add padding\\n        return_attention_mask=True,\\n    )\\n\\n    # APPEND inputs token and input masks. YOUR CODE HERE\\n    inputs_tokens_test.append(string_tokenized[\\\"input_ids\\\"])\\n    attention_masks_test.append(string_tokenized[\\\"attention_mask\\\"])\")\n","            })();\n","            "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["tokenizing train dataset...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25000/25000 [00:36<00:00, 693.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["tokenizing test dataset...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25000/25000 [00:33<00:00, 739.89it/s]\n"]}],"source":["import numpy as np\n","\n","maxL = 512\n","temb = 768\n","\n","inputs_tokens_train = []\n","attention_masks_train = []\n","\n","print(\"tokenizing train dataset...\")\n","for i in tqdm(range(len(train))):\n","    # if(i%2500==0):\n","    #     print(i)\n","    string_tokenized = tokenizer.encode_plus(train[i][0], return_tensors=\"pt\", \n","                                             add_special_tokens=True,  # add '[CLS]' and '[SEP]'\n","                                             max_length=maxL,  # set max length\n","                                             truncation=True,  # truncate longer messages\n","                                             # pad_to_max_length=True\n","                                             padding='max_length',  # add padding\n","                                             return_attention_mask=True)\n","    \n","    # APPEND inputs token and input masks. YOUR CODE HERE\n","    inputs_tokens_train.append(string_tokenized['input_ids'])\n","    attention_masks_train.append(string_tokenized[\"attention_mask\"])\n","\n","inputs_tokens_test = []\n","attention_masks_test = []\n","\n","print(\"tokenizing test dataset...\")\n","for i in tqdm(range(len(test))):\n","    # if(i%2500==0):\n","    #     print(i)\n","    string_tokenized = tokenizer.encode_plus(test[i][0], return_tensors=\"pt\", \n","                                             add_special_tokens=True,  # add '[CLS]' and '[SEP]'\n","                                             max_length=maxL,  # set max length\n","                                             truncation=True,  # truncate longer messages\n","                                             # pad_to_max_length=True\n","                                             padding='max_length',  # add padding\n","                                             return_attention_mask=True)\n","    \n","    # APPEND inputs token and input masks. YOUR CODE HERE\n","    inputs_tokens_test.append(string_tokenized['input_ids'])\n","    attention_masks_test.append(string_tokenized[\"attention_mask\"])"]},{"cell_type":"markdown","source":["# Let's create a 'TensorDataSet' **for the training samples**\n","\n","Each element is a triplet composed of token word index, token mask, and label"],"metadata":{"id":"lZrSrBuS-HnW"},"id":"lZrSrBuS-HnW"},{"cell_type":"code","execution_count":25,"id":"93880db1","metadata":{"id":"93880db1","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1676642190823,"user_tz":-60,"elapsed":1033,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"e2f5f0b1-0b1e-45f5-c49c-7ed0c2e44724"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","            (function() {\n","                jb_set_cell(\"# Converting input tokens to torch tensors\\ninputs_tokens_train = torch.cat(inputs_tokens_train, dim=0)\\nattention_masks_train = torch.cat(attention_masks_train, dim=0)\\n\\n# Converting labels to numpy then torch tensor\\ny_train = np.zeros((len(train),))\\nfor i in range(len(train)):\\n    y_train[i] = train[i][1]\\ny_train = torch.from_numpy(y_train)\\n\\nfrom torch.utils.data import (\\n    TensorDataset,\\n    random_split,\\n    DataLoader,\\n    RandomSampler,\\n    SequentialSampler,\\n)\\n\\ntrain_dataset = TensorDataset(inputs_tokens_train, attention_masks_train, y_train)\")\n","            })();\n","            "]},"metadata":{}}],"source":["# Converting input tokens to torch tensors \n","inputs_tokens_train = torch.cat(inputs_tokens_train, dim=0)\n","attention_masks_train = torch.cat(attention_masks_train, dim=0)\n","\n","# Converting labels to numpy then torch tensor\n","y_train = np.zeros((len(train),))\n","for i in range(len(train)):\n","  y_train[i] = train[i][1]\n","y_train = torch.from_numpy(y_train)\n","\n","from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n","train_dataset = TensorDataset(inputs_tokens_train, attention_masks_train, y_train)"]},{"cell_type":"markdown","source":["# Let's do the same **for the test samples**"],"metadata":{"id":"BpuQLQEm_WSC"},"id":"BpuQLQEm_WSC"},{"cell_type":"code","execution_count":26,"id":"88411b6a","metadata":{"id":"88411b6a","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1676642272740,"user_tz":-60,"elapsed":506,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"4159a9a4-5c9f-4b49-a876-11cf6c9fdb64"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","            (function() {\n","                jb_set_cell(\"# Converting input tokens to torch tensors\\ninputs_tokens_test = torch.cat(inputs_tokens_test, dim=0)\\nattention_masks_test = torch.cat(attention_masks_test, dim=0)\\n\\ny_test = np.zeros((len(test),))\\nfor i in range(len(test)):\\n    y_test[i] = test[i][1]\\ny_test = torch.from_numpy(y_test)\\n\\ntest_dataset = TensorDataset(inputs_tokens_test, attention_masks_test, y_test)\")\n","            })();\n","            "]},"metadata":{}}],"source":["# Converting input tokens to torch tensors \n","inputs_tokens_test = torch.cat(inputs_tokens_test, dim=0)\n","attention_masks_test = torch.cat(attention_masks_test, dim=0)\n","  \n","y_test = np.zeros((len(test),))\n","for i in range(len(test)):\n","    y_test[i] = test[i][1]\n","y_test = torch.from_numpy(y_test)\n","\n","test_dataset = TensorDataset(inputs_tokens_test, attention_masks_test, y_test)"]},{"cell_type":"code","source":["# If you need to clean GPU memory\n","import gc\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"1bX5vtWb_vj8","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1676644081030,"user_tz":-60,"elapsed":624,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"12b84c04-04a7-4060-a4e9-ec0347052548"},"id":"1bX5vtWb_vj8","execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","            (function() {\n","                jb_set_cell(\"# If you need to clean GPU memory\\nimport gc\\n\\ngc.collect()\\ntorch.cuda.empty_cache()\")\n","            })();\n","            "]},"metadata":{}}]},{"cell_type":"markdown","source":["# Most important STEP\n","\n","We want to extract the [CLS] representation (1st token of the last layer before logits) for each review, and store it in train and test.  "],"metadata":{"id":"FoDmvAIO_6l8"},"id":"FoDmvAIO_6l8"},{"cell_type":"code","source":["!pip install tqdm"],"metadata":{"id":"vtKShCs8wNr5"},"id":"vtKShCs8wNr5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm"],"metadata":{"id":"VmNHpA7_uqLa","executionInfo":{"status":"ok","timestamp":1676644121474,"user_tz":-60,"elapsed":517,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}}},"id":"VmNHpA7_uqLa","execution_count":38,"outputs":[]},{"cell_type":"code","execution_count":36,"id":"90ee3dc0","metadata":{"id":"90ee3dc0","colab":{"base_uri":"https://localhost:8080/","height":559},"executionInfo":{"status":"ok","timestamp":1676643974761,"user_tz":-60,"elapsed":750307,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"4959fc28-0f1d-48aa-dc84-fcdab2be2571"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","            (function() {\n","                jb_set_cell(\"# create DataLoaders with samplers\\ntb = int(100)\\ntrain_dataloader = DataLoader(train_dataset, batch_size=tb, shuffle=False)\\nnbTrain = len(train)  # nb d'exemples en train\\nf_train = np.zeros(\\n    (nbTrain, temb)\\n)  # features de train (les classes CLS, ce qu'on stocke et ce qu'on va calculer)\\nnbtach = int(nbTrain / tb)\\nprint(f\\\"nb batches={nbtach}\\\")\\n\\n# Comuting CLS features\\nmodel.cuda()\\nfor idx, batch in enumerate(train_dataloader):\\n    # Unpack this training batch from our dataloader:\\n    # `batch` contains three pytorch tensors:\\n    #   [0]: input ids\\n    #   [1]: attention masks\\n    #   [2]: labels\\n    if idx % 10 == 0:\\n        print(f\\\"batch {idx} / {nbtach}\\\")\\n    b_input_ids = batch[0].cuda()\\n    b_input_mask = batch[1].cuda()\\n    b_labels = batch[2].cuda().long()\\n\\n    with torch.no_grad():  # on ne cherche pas \\u00e0 faire de l'apprentissage donc on ne calcule pas les gradients (pour aller plus vite et \\u00e9conomiser de la m\\u00e9moire)\\n        # forward propagation (evaluate model on training batch)\\n        output = model(\\n            input_ids=b_input_ids,\\n            attention_mask=b_input_mask,\\n            # labels=b_labels,\\n            output_hidden_states=True,\\n        )\\n        last_hidden_states = output.hidden_states[\\n            -1\\n        ]  # WARNING: it is now a batch of size tbatch x nToken x embsize (100*512*768)\\n        # tb \\u00e9l\\u00e9ments d'entrainement\\n        f_train[idx * tb : idx * tb + tb, :] = last_hidden_states.detach().cpu().numpy()[:, 0, :]\\n        # YOUR CODE HERE. Think in applying .detach().cpu().numpy()\")\n","            })();\n","            "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["nb batches=250\n","batch 0 / 250\n","batch 10 / 250\n","batch 20 / 250\n","batch 30 / 250\n","batch 40 / 250\n","batch 50 / 250\n","batch 60 / 250\n","batch 70 / 250\n","batch 80 / 250\n","batch 90 / 250\n","batch 100 / 250\n","batch 110 / 250\n","batch 120 / 250\n","batch 130 / 250\n","batch 140 / 250\n","batch 150 / 250\n","batch 160 / 250\n","batch 170 / 250\n","batch 180 / 250\n","batch 190 / 250\n","batch 200 / 250\n","batch 210 / 250\n","batch 220 / 250\n","batch 230 / 250\n","batch 240 / 250\n"]}],"source":["# create DataLoaders with samplers\n","tb = int(100)\n","train_dataloader = DataLoader(train_dataset, batch_size=tb,shuffle=False)\n","nbTrain = len(train) # nb d'exemples en train\n","f_train = np.zeros((nbTrain, temb)) # features de train (les classes CLS, ce qu'on stocke et ce qu'on va calculer)\n","nbtach = int(nbTrain/tb)\n","print(f\"nb batches = {nbtach}\")\n","\n","# Comuting CLS features\n","model.cuda()\n","for idx, batch in enumerate(tqdm(train_dataloader)):\n","        # Unpack this training batch from our dataloader:\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids\n","        #   [1]: attention masks\n","        #   [2]: labels\n","        # if(idx%10==0):\n","        #     print(f\"batch {idx} / {nbtach}\")\n","        b_input_ids = batch[0].cuda()\n","        b_input_mask = batch[1].cuda()\n","        b_labels = batch[2].cuda().long()\n","        \n","        with torch.no_grad(): # on ne cherche pas à faire de l'apprentissage donc on ne calcule pas les gradients (pour aller plus vite et économiser de la mémoire)\n","            # forward propagation (evaluate model on training batch)\n","            output = model(input_ids=b_input_ids,\n","                           attention_mask=b_input_mask,\n","                           # labels=b_labels, \n","                           output_hidden_states=True)\n","            last_hidden_states = output.hidden_states[-1] # WARNING: it is now a batch of size tbatch x nToken x embsize (100*512*768)\n","            # tb éléments d'entrainement\n","            # YOUR CODE HERE. Think in applying .detach().cpu().numpy()\n","            f_train[idx*tb:idx*tb+tb,:] = last_hidden_states.detach().cpu().numpy()[:,0,:]\n"]},{"cell_type":"markdown","source":["# Extract [CLS] token in TEST"],"metadata":{"id":"srd5tJqyJSRq"},"id":"srd5tJqyJSRq"},{"cell_type":"code","execution_count":39,"id":"0aafb30d-357b-46da-a1f8-bfd18832ba24","metadata":{"id":"0aafb30d-357b-46da-a1f8-bfd18832ba24","colab":{"base_uri":"https://localhost:8080/","height":59},"executionInfo":{"status":"ok","timestamp":1676644902419,"user_tz":-60,"elapsed":769891,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"bb43a849-5e79-415c-f5f4-e077b2d5e85e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","            (function() {\n","                jb_set_cell(\"# create DataLoaders with samplers\\ntb = int(100)\\ntest_dataloader = DataLoader(test_dataset, batch_size=tb, shuffle=False)\\nnbTest = len(test)\\nf_test = np.zeros((nbTest, temb))\\nnbtach = int(nbTest / tb)\\nprint(f\\\"nb batches={nbtach}\\\")\\n# Comuting CLS features\\nmodel.cuda()\\nfor idx, batch in enumerate(tqdm(test_dataloader)):\\n    # Unpack this training batch from our dataloader:\\n    # `batch` contains three pytorch tensors:\\n    #   [0]: input ids\\n    #   [1]: attention masks\\n    #   [2]: labels\\n    # if(idx%10==0):\\n    #     print(f\\\"batch {idx} / {nbtach}\\\")\\n    b_input_ids = batch[0].cuda()\\n    b_input_mask = batch[1].cuda()\\n    b_labels = batch[2].cuda().long()\\n\\n    with torch.no_grad():\\n        # forward propagation (evaluate model on training batch)\\n        output = model(\\n            input_ids=b_input_ids,\\n            attention_mask=b_input_mask,\\n            # labels=b_labels,\\n            output_hidden_states=True,\\n        )\\n        last_hidden_states = output.hidden_states[-1]  # YOUR CODE HERE.\\n        #\\n        f_test[idx * tb : idx * tb + tb, :] = (\\n            last_hidden_states.detach().cpu().numpy()[:, 0, :]\\n        )  # YOUR CODE HERE.\")\n","            })();\n","            "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["nb batches=250\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 250/250 [12:49<00:00,  3.08s/it]\n"]}],"source":["# create DataLoaders with samplers\n","tb = int(100)\n","test_dataloader = DataLoader(test_dataset, batch_size=tb,shuffle=False)\n","nbTest = len(test)\n","f_test = np.zeros((nbTest, temb))\n","nbtach = int(nbTest/tb)\n","print(f\"nb batches={nbtach}\")\n","# Comuting CLS features\n","model.cuda()\n","for idx,batch in enumerate(tqdm(test_dataloader)):\n","        # Unpack this training batch from our dataloader:\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids\n","        #   [1]: attention masks\n","        #   [2]: labels\n","        # if(idx%10==0):\n","        #     print(f\"batch {idx} / {nbtach}\")\n","        b_input_ids = batch[0].cuda()\n","        b_input_mask = batch[1].cuda()\n","        b_labels = batch[2].cuda().long()\n","        \n","        with torch.no_grad():\n","            # forward propagation (evaluate model on training batch)\n","            output = model(input_ids=b_input_ids,\n","                                 attention_mask=b_input_mask,\n","                                 #labels=b_labels, \n","                               output_hidden_states=True)\n","            last_hidden_states = output.hidden_states[-1]# YOUR CODE HERE.\n","            #\n","            f_test[idx*tb:idx*tb+tb,:] = last_hidden_states.detach().cpu().numpy()[:,0,:] # YOUR CODE HERE.\n","        "]},{"cell_type":"markdown","source":["# Now save the embedding of each review into disk!"],"metadata":{"id":"AReISeiZIo9U"},"id":"AReISeiZIo9U"},{"cell_type":"code","execution_count":40,"id":"deb81156-d885-4999-a6e7-2d8149173174","metadata":{"id":"deb81156-d885-4999-a6e7-2d8149173174","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1676644939966,"user_tz":-60,"elapsed":2167,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"6dab2aa3-c02e-460f-e18f-5c005fb5dd24"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","            (function() {\n","                jb_set_cell(\"# Saving the features and labels\\nimport pickle\\n\\n# Open a file and use dump()\\nwith open(\\\"train-data.pkl\\\", \\\"wb\\\") as file:\\n    # A new file will be created\\n    pickle.dump([f_train, y_train], file)\\n\\nwith open(\\\"test-data.pkl\\\", \\\"wb\\\") as file:\\n    # A new file will be created\\n    pickle.dump([f_test, y_test], file)\")\n","            })();\n","            "]},"metadata":{}}],"source":["# Saving the features and labels\n","import pickle\n","# Open a file and use dump()\n","with open('train-data.pkl', 'wb') as file:\n","    # A new file will be created\n","    pickle.dump([f_train,y_train], file)\n","\n","with open('test-data.pkl', 'wb') as file:\n","    # A new file will be created\n","    pickle.dump([f_test,y_test], file)  "]},{"cell_type":"code","execution_count":41,"id":"67f5f29e","metadata":{"id":"67f5f29e","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1676644944568,"user_tz":-60,"elapsed":11,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"6c1e6e25-63d5-4156-ae5f-7e8257a38fe4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","            (function() {\n","                jb_set_cell(\"import pickle\\n\\n# Open the file in binary mode\\nwith open(\\\"train-data.pkl\\\", \\\"rb\\\") as file:\\n    # Call load method to deserialze\\n    [feature_train, ytrain] = pickle.load(file)\\n\\n# Open the file in binary mode\\nwith open(\\\"test-data.pkl\\\", \\\"rb\\\") as file:\\n    # Call load method to deserialze\\n    [feature_test, ytest] = pickle.load(file)\")\n","            })();\n","            "]},"metadata":{}}],"source":["import pickle\n","  \n","# Open the file in binary mode\n","with open('train-data.pkl', 'rb') as file:    \n","    # Call load method to deserialze\n","    [feature_train, ytrain] = pickle.load(file)\n","\n","# Open the file in binary mode\n","with open('test-data.pkl', 'rb') as file:    \n","    # Call load method to deserialze\n","    [feature_test, ytest] = pickle.load(file)  \n","    "]},{"cell_type":"code","execution_count":42,"id":"4104eea1","metadata":{"id":"4104eea1","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1676644948670,"user_tz":-60,"elapsed":519,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"d0eddeb5-1869-4fbc-8fb9-80c9b3823b3f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","            (function() {\n","                jb_set_cell(\"import numpy as np\\n\\nprint(feature_train.shape[0])\\nprint(feature_test.shape)\\n\\nprint(ytrain)\\nprint(ytest)\\nprint(np.linalg.norm(feature_train[10]))\")\n","            })();\n","            "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["25000\n","(25000, 768)\n","tensor([1., 1., 1.,  ..., 0., 0., 0.], dtype=torch.float64)\n","tensor([1., 1., 1.,  ..., 0., 0., 0.], dtype=torch.float64)\n","15.1259890017741\n"]}],"source":["import numpy as np\n","print(feature_train.shape[0])\n","print(feature_test.shape)\n","\n","print(ytrain)\n","print(ytest)\n","print(np.linalg.norm(feature_train[10]))"]},{"cell_type":"markdown","source":["# Finally: train a logistic regression model on top of extracted embeddings. Conclude on the performances of BERT for the sentiment classification task"],"metadata":{"id":"mx0eec9RIwJr"},"id":"mx0eec9RIwJr"},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","\n","lr = LogisticRegression().fit(feature_train, ytrain)\n","lr.score(feature_test, ytest)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_xJ2kNbt08yy","executionInfo":{"status":"ok","timestamp":1676645181045,"user_tz":-60,"elapsed":4548,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"2fa371d3-c1ca-4a91-c92c-bdf0a28c29a1"},"id":"_xJ2kNbt08yy","execution_count":44,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"execute_result","data":{"text/plain":["0.88516"]},"metadata":{},"execution_count":44}]},{"cell_type":"markdown","source":["Quasi 90% c'est plutôt pas mal hein, en transfert pur. On pourrait faire du fine tuning, reprendre BERT et l'optimiser sur oles labels de la tache cible et réoptimiser, on devrait trouver ENCORE mieux.\n","\n","Modèle très lourd, très puissant mais le gap est surout marqué dans les taches bcp plus fines par rapport aux autres modèles. "],"metadata":{"id":"aUbA4SCe2MYo"},"id":"aUbA4SCe2MYo"},{"cell_type":"code","source":["lr = LogisticRegression(C=100000).fit(feature_train, ytrain)\n","print(lr.score(feature_test, ytest))\n","print(lr.score(feature_train, ytrain))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9JCw_p1a1u8h","executionInfo":{"status":"ok","timestamp":1676645727126,"user_tz":-60,"elapsed":5127,"user":{"displayName":"Aymeric Delefosse","userId":"08495968576830521594"}},"outputId":"30690516-32e2-46c8-fdce-d0c345540614"},"id":"9JCw_p1a1u8h","execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["0.886\n","0.88784\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"w4UAgrnv2_qV"},"id":"w4UAgrnv2_qV","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"colab":{"provenance":[{"file_id":"1S8ijD0HNVpDaNZFvOMiltj7qi_N6jf8Z","timestamp":1676639437290},{"file_id":"1a8Pro5FG8LgO0jtypN_o6vL65YWF5-Zf","timestamp":1676565988413},{"file_id":"1H6b5gJT0mW7AJOAEVxiIN6q2v8fQl_yG","timestamp":1676559453001}]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}